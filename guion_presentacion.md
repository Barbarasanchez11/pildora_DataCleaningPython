# üéØ Gui√≥n Completo: P√≠ldora Data Cleaning Avanzado en Python

## üìã Informaci√≥n General
- **Duraci√≥n**: 30 minutos (25 min presentaci√≥n + 5 min preguntas)
- **Audiencia**: Desarrolladores/Data Scientists con conocimientos b√°sicos de Python
- **Objetivo**: Mostrar t√©cnicas avanzadas de limpieza de datos para proyectos reales

---

## üöÄ Introducci√≥n (2 minutos)

### Saludo y Contexto
"Hola a todos! üëã Mi compa√±ero nos ha mostrado los fundamentos del data cleaning. Ahora vamos a ver t√©cnicas m√°s avanzadas que realmente marcan la diferencia en la calidad de tus an√°lisis."

### Puntos clave a transmitir:
- ‚úÖ Estas son t√©cnicas del mundo real que usar√°s en proyectos
- ‚úÖ Van m√°s all√° de `dropna()` y `fillna()`
- ‚úÖ La diferencia entre un an√°lisis mediocre y uno profesional
- ‚úÖ 30 minutos que cambiar√°n tu forma de trabajar con datos

### Agenda visual:
```
üìä T√©cnicas Avanzadas (15 min)
   ‚îú‚îÄ‚îÄ Detecci√≥n inteligente de outliers
   ‚îú‚îÄ‚îÄ Limpieza de texto con regex
   ‚îî‚îÄ‚îÄ Validaci√≥n de datos
üéØ Casos Pr√°cticos (10 min)
   ‚îú‚îÄ‚îÄ E-commerce con m√∫ltiples problemas
   ‚îî‚îÄ‚îÄ Series temporales complejas
‚öôÔ∏è Automatizaci√≥n (5 min)
   ‚îú‚îÄ‚îÄ Pipeline reutilizable
   ‚îî‚îÄ‚îÄ Buenas pr√°cticas
```

## Parte 1: Detecci√≥n Inteligente de Outliers (8 min)

### A) M√©todos Estad√≠sticos Tradicionales (3 min)
```python
# Mostrar Z-score y IQR
def detectar_outliers_zscore(df, columnas, threshold=3):
    # C√≥digo para Z-score
    pass

def detectar_outliers_iqr(df, columnas, factor=1.5):
    # C√≥digo para IQR
    pass
```

**Puntos a destacar:**
- Z-score: Mejor para datos normalmente distribuidos
- IQR: M√°s robusto para datos sesgados
- Mostrar cu√°ndo fallan estos m√©todos

### B) Isolation Forest para Outliers Multivariados (5 min)
```python
# C√≥digo pr√°ctico con ejemplos reales
def detectar_outliers_isolation_forest(df, columnas, contamination=0.1):
    iso_forest = IsolationForest(contamination=contamination, random_state=42)
    outliers = iso_forest.fit_predict(df[columnas])
    return outliers
```

**Puntos a destacar:**
- Mejor para m√∫ltiples variables
- Detecta outliers complejos
- Configuraci√≥n del par√°metro contamination

## Parte 2: Limpieza de Texto Avanzada (7 min)

### A) Expresiones Regulares para Patrones Complejos (4 min)
```python
# Datos sucios t√≠picos de CRM
clientes_sucios = pd.DataFrame({
    'telefono': ['+34-666-123-456', '666 123 456', '666.123.456'],
    'email': ['JUAN@GMAIL.COM', 'ana@Yahoo.es  ', 'pedro@hotmail'],
    'nombre': ['  Juan P√©rez  ', 'ANA GARC√çA', 'pedro l√≥pez']
})

def limpiar_telefonos(telefono):
    numeros = re.sub(r'[^\d]', '', str(telefono))
    if numeros.startswith('34'):
        numeros = numeros[2:]
    return f"+34-{numeros[:3]}-{numeros[3:6]}-{numeros[6:9]}" if len(numeros) == 9 else None
```

**Puntos a destacar:**
- Patrones regex espec√≠ficos para cada tipo
- Validaci√≥n de formatos
- Normalizaci√≥n consistente

### B) Manejo de Encoding y HTML (3 min)
```python
def limpiar_html(texto):
    # Eliminar etiquetas HTML
    texto_limpio = re.sub(r'<[^>]+>', '', str(texto))
    # Decodificar entidades HTML
    entidades = {'&amp;': '&', '&lt;': '<', '&gt;': '>'}
    for entidad, caracter in entidades.items():
        texto_limpio = texto_limpio.replace(entidad, caracter)
    return texto_limpio.strip()
```

## Parte 3: Casos Pr√°cticos Complejos (10 min)

### Caso 1: Dataset de E-commerce (5 min)
```python
# Ejemplo: Limpiar datos de productos con m√∫ltiples problemas
- Precios en diferentes monedas
- Categor√≠as inconsistentes  
- Descripciones con HTML/caracteres especiales
```

**Problemas a resolver:**
- Normalizar precios a una moneda base
- Estandarizar categor√≠as
- Limpiar descripciones HTML

### Caso 2: Datos Temporales (5 min)
```python
# Manejo de series temporales sucias
- Timestamps en diferentes zonas horarias
- Frecuencias irregulares
- Gaps temporales
```

**Problemas a resolver:**
- Parsear fechas en diferentes formatos
- Normalizar zonas horarias
- Rellenar gaps temporales

## Parte 4: Automatizaci√≥n y Buenas Pr√°cticas (5 min)

### A) Pipeline de Limpieza Reutilizable (3 min)
```python
class DataCleaner:
    def __init__(self):
        self.transformations = []
    
    def fit_transform(self, df):
        # 1. Eliminar duplicados exactos
        # 2. Estandarizar columnas de texto
        # 3. Manejar valores nulos con estrategia inteligente
        # 4. Log de transformaciones
        pass
```

### B) Logging y Testing (2 min)
```python
# Logging de transformaciones
print(f"Filas eliminadas (duplicados): {len(df) - len(df_clean)}")
print(f"Valores nulos imputados: {df.isnull().sum().sum()}")

# Testing de calidad de datos
def validar_calidad(df):
    assert df.duplicated().sum() == 0, "Hay duplicados"
    assert df.isnull().sum().sum() == 0, "Hay valores nulos"
    return True
```

## Conclusi√≥n (3 min)

### Puntos clave:
- Resumir t√©cnicas aprendidas
- Enfatizar la importancia de la automatizaci√≥n
- Mencionar pr√≥ximos pasos
- Abrir para preguntas

### Frases de cierre:
"Estas son las t√©cnicas que realmente marcan la diferencia en la calidad de tus an√°lisis. La clave est√° en automatizar, documentar y validar todo lo que haces."

## Notas para el Presentador

### Antes de empezar:
- Verificar que todos los m√≥dulos se importan correctamente
- Tener datos de ejemplo preparados
- Probar la visualizaci√≥n de gr√°ficos

### Durante la presentaci√≥n:
- Pausar para preguntas despu√©s de cada parte
- Mostrar el c√≥digo en vivo cuando sea posible
- Explicar el "por qu√©" detr√°s de cada t√©cnica

### Si algo falla:
- Tener versiones simplificadas de los ejemplos
- Explicar el concepto aunque el c√≥digo no funcione
- Mantener la calma y seguir con la presentaci√≥n

### Tiempo estimado:
- Introducci√≥n: 2 min
- Parte 1: 8 min
- Parte 2: 7 min
- Parte 3: 10 min
- Parte 4: 5 min
- Conclusi√≥n: 3 min
- **Total: 35 minutos** (con 5 min de buffer para preguntas)
